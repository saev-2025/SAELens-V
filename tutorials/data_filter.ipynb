{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, load_dataset,load_from_disk\n",
    "import hashlib\n",
    "import os\n",
    "import tqdm\n",
    "file_path = \"\"\n",
    "\n",
    "data = torch.load(file_path)\n",
    "\n",
    "dataset_path=\"\"\n",
    "train_dataset = load_dataset(\n",
    "            dataset_path,\n",
    "            split=\"train\",\n",
    "            trust_remote_code=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_keep = [ 'id',\"image\", \"conversations\"]\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([col for col in train_dataset.column_names if col not in columns_to_keep])\n",
    "\n",
    "print(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list=[]\n",
    "for data_l in data:\n",
    "    keys=list(data_l.keys())\n",
    "    for key in keys:\n",
    "        data_list.append((key,data_l[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_list))\n",
    "print(data_list[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "for j, item in tqdm.tqdm(enumerate(train_dataset), desc=\"Processing data_list\", unit=\"item\"):\n",
    "    key = generate_text_hash(item['conversations'][0][\"value\"]+item['conversations'][1][\"value\"])\n",
    "    if key not in dataset_dict:\n",
    "        dataset_dict[key] = []\n",
    "    dataset_dict[key].append(j) \n",
    "\n",
    "formatted_dataset = []\n",
    "index_set = set() \n",
    "\n",
    "for i in tqdm.tqdm(range(len(data_list)), desc=\"Processing data_list\", unit=\"item\"):\n",
    "    key_datalist, value_datalist = data_list[i]\n",
    "    \n",
    "    if key_datalist in dataset_dict:\n",
    "        for j in dataset_dict[key_datalist]:\n",
    "            if j not in index_set:  \n",
    "                new_item = train_dataset[j].copy() \n",
    "                new_item[\"cooccur_score\"] = value_datalist\n",
    "                formatted_dataset.append(new_item)\n",
    "                \n",
    "                index_set.add(j)\n",
    "                break  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(formatted_dataset))\n",
    "print(formatted_dataset[0][\"cooccur_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cosi_file_path = \"\"  \n",
    "osi_dict = {}\n",
    "with open(cosi_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        key, value = line.strip().split(\",\")  \n",
    "        osi_dict[int(key)] = float(value)    \n",
    "\n",
    "print(osi_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cosi_key=set(osi_dict.keys())\n",
    "cosi_coocur_data={}\n",
    "for f_data in tqdm.tqdm(formatted_dataset):\n",
    "    score=0\n",
    "    for value in f_data['cooccur_score']:\n",
    "        if value in set_cosi_key:\n",
    "            score+=osi_dict[value]\n",
    "    f_data[\"Cooccur_score\"]=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset[-1][\"Cooccur_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(formatted_dataset)\n",
    "dataset.save_to_disk(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in formatted_dataset:\n",
    "    item[\"l0\"]=float(item[\"l0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset_sorted = sorted(formatted_dataset, key=lambda x: x[\"Cooccur_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formatted_dataset_sorted[0][\"Cooccur_score\"])\n",
    "print(formatted_dataset_sorted[-1][\"Cooccur_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_samples = len(formatted_dataset_sorted)\n",
    "q1 = int(num_samples * 0.25)\n",
    "q2 = int(num_samples * 0.5)\n",
    "q3 = int(num_samples * 0.75)\n",
    "\n",
    "split_datasets = {\n",
    "    \"q0_25\": formatted_dataset_sorted[:q1],\n",
    "    \"q25_50\": formatted_dataset_sorted[q1:q2],\n",
    "    \"q50_75\": formatted_dataset_sorted[q2:q3],\n",
    "    \"q75_100\": formatted_dataset_sorted[q3:]\n",
    "}\n",
    "\n",
    "hf_datasets = {}\n",
    "for split_name, split_data in tqdm.tqdm(split_datasets.items(), desc=\"Processing splits\", unit=\"split\"):\n",
    "    formatted_data = {}\n",
    "    \n",
    "    for key in tqdm.tqdm(split_data[0].keys(), desc=f\"Formatting {split_name}\", unit=\"column\",position=1):\n",
    "        formatted_data[key] = [d[key] for d in split_data]\n",
    "    \n",
    "    hf_datasets[split_name] = Dataset.from_dict(formatted_data)\n",
    "\n",
    "    hf_datasets[split_name].save_to_disk(f\"./{split_name}_dataset\")\n",
    "\n",
    "for split_name, dataset in hf_datasets.items():\n",
    "    print(f\"{split_name} dataset:\")\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from collections import Counter\n",
    "\n",
    "values_list=[float(tensor[\"Cooccur_score\"]) for tensor in formatted_dataset_sorted]\n",
    "num_bins = 10\n",
    "\n",
    "frequencies, bin_edges = np.histogram(values_list, bins=num_bins)\n",
    "\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "bin_centers_smooth = np.linspace(bin_centers[0], bin_centers[-1], 300)  \n",
    "frequencies_smooth = make_interp_spline(bin_centers, frequencies)(bin_centers_smooth)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(bin_centers_smooth, frequencies_smooth, color='orange', lw=2)\n",
    "\n",
    "\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.xlabel('cosine similarity score of data ', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.title('The distribution of Compcap data based on cosine similarity score', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
